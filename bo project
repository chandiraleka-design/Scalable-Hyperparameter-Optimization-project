{"cells":[{"cell_type":"markdown","metadata":{"id":"Wf5KrEb6vrkR"},"source":["# Welcome to Colab!"]},{"cell_type":"code","source":["\"\"\"\n","Bayesian Optimization Project\n","Scalable Hyperparameter Optimization via Bayesian Optimization with Custom Acquisition Functions\n","\n","Tasks:\n","- Task 1: Implement Gaussian Process regression model + kernels (RBF, Matern 5/2) and posterior.\n","- Task 2: Implement acquisition functions (Expected Improvement, Upper Confidence Bound).\n","- Task 3: Integrate GP + acquisition into Bayesian Optimization loop for hyperparameter tuning.\n","- Task 4: Comparative analysis vs Random Search, reporting convergence and final performance.\n","\n","Beginner-friendly, documented, and modular.\n","\"\"\"\n","\n","import numpy as np\n","from dataclasses import dataclass\n","from typing import Callable, Dict, Tuple, List\n","from scipy.linalg import cho_factor, cho_solve\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.metrics import mean_squared_error\n","import warnings\n","\n","# -----------------------\n","# Utility: Reproducibility\n","# -----------------------\n","RNG_SEED = 42\n","rng = np.random.default_rng(RNG_SEED)\n","np.random.seed(RNG_SEED)\n","\n","# ============================================================\n","# Task 1: Implement Gaussian Process regression model + kernels\n","# ============================================================\n","\n","@dataclass\n","class KernelParams:\n","    lengthscale: float = 1.0\n","    variance: float = 1.0\n","\n","def rbf_kernel(X: np.ndarray, Y: np.ndarray, params: KernelParams) -> np.ndarray:\n","    \"\"\"\n","    RBF (Squared Exponential) kernel.\n","    K(x, y) = variance * exp(- ||x - y||^2 / (2 * lengthscale^2))\n","    \"\"\"\n","    X_sq = np.sum(X**2, axis=1, keepdims=True)\n","    Y_sq = np.sum(Y**2, axis=1, keepdims=True).T\n","    cross = X @ Y.T\n","    dists = X_sq + Y_sq - 2.0 * cross\n","    return params.variance * np.exp(-0.5 * dists / (params.lengthscale**2))\n","\n","def matern52_kernel(X: np.ndarray, Y: np.ndarray, params: KernelParams) -> np.ndarray:\n","    \"\"\"\n","    Matern 5/2 kernel.\n","    K(r) = variance * (1 + sqrt(5) r / l + 5 r^2 / (3 l^2)) * exp(-sqrt(5) r / l)\n","    \"\"\"\n","    X_sq = np.sum(X**2, axis=1, keepdims=True)\n","    Y_sq = np.sum(Y**2, axis=1, keepdims=True).T\n","    cross = X @ Y.T\n","    d2 = np.maximum(X_sq + Y_sq - 2.0 * cross, 0.0)\n","    r = np.sqrt(d2)\n","    l = params.lengthscale\n","    sqrt5_r_l = np.sqrt(5.0) * r / l\n","    term = (1.0 + sqrt5_r_l + 5.0 * (r**2) / (3.0 * l**2))\n","    return params.variance * term * np.exp(-sqrt5_r_l)\n","\n","class GaussianProcess:\n","    \"\"\"\n","    Gaussian Process regressor for scalar outputs.\n","\n","    - Supports RBF and Matern 5/2 kernels (Task 1).\n","    - Posterior via Cholesky for numerical stability (Task 1).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        kernel_fn: Callable[[np.ndarray, np.ndarray, KernelParams], np.ndarray],\n","        kernel_params: KernelParams,\n","        noise_variance: float = 1e-6,\n","        jitter: float = 1e-8,\n","    ):\n","        self.kernel_fn = kernel_fn\n","        self.kernel_params = kernel_params\n","        self.noise_variance = noise_variance\n","        self.jitter = jitter\n","        self.X_train = None\n","        self.y_train = None\n","        self._cho = None\n","        self._alpha = None\n","\n","    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n","        \"\"\"\n","        Fit GP: compute K + sigma^2 I and its Cholesky, plus alpha = (K + sigma^2 I)^{-1} y.\n","        \"\"\"\n","        self.X_train = np.array(X, dtype=np.float64)\n","        self.y_train = np.array(y, dtype=np.float64).reshape(-1)\n","\n","        K = self.kernel_fn(self.X_train, self.X_train, self.kernel_params)\n","        K[np.diag_indices_from(K)] += (self.noise_variance + self.jitter)\n","\n","        try:\n","            c, lower = cho_factor(K, check_finite=False)\n","        except np.linalg.LinAlgError:\n","            warnings.warn(\"Cholesky failed; adding extra jitter.\")\n","            K[np.diag_indices_from(K)] += 1e-6\n","            c, lower = cho_factor(K, check_finite=False)\n","\n","        self._cho = (c, lower)\n","        self._alpha = cho_solve(self._cho, self.y_train, check_finite=False)\n","\n","    def predict(self, X_star: np.ndarray, return_var: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Posterior mean and variance at test points X_star.\n","        \"\"\"\n","        if self.X_train is None:\n","            raise ValueError(\"GP not fitted.\")\n","\n","        K_star = self.kernel_fn(self.X_train, X_star, self.kernel_params)  # (n_train, n_star)\n","        mu = K_star.T @ self._alpha\n","\n","        if not return_var:\n","            return mu, None\n","\n","        v = cho_solve(self._cho, K_star, check_finite=False)\n","        K_star_star = self.kernel_fn(X_star, X_star, self.kernel_params)\n","        var = np.maximum(np.diag(K_star_star) - np.sum(K_star * v, axis=0), 0.0)\n","        return mu, var\n","\n","# ============================================================\n","# Task 2: Implement acquisition functions (EI and UCB)\n","# ============================================================\n","\n","def expected_improvement(mu: np.ndarray, sigma2: np.ndarray, best_f: float, xi: float = 1e-3) -> np.ndarray:\n","    \"\"\"\n","    Expected Improvement (maximize) (Task 2).\n","    EI = E[max(0, f(x) - best_f - xi)]\n","    \"\"\"\n","    sigma = np.sqrt(np.maximum(sigma2, 0.0))\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        z = (mu - best_f - xi) / (sigma + 1e-12)\n","        from scipy.stats import norm\n","        ei = (mu - best_f - xi) * norm.cdf(z) + (sigma) * norm.pdf(z)\n","        ei[sigma < 1e-12] = 0.0\n","    return ei\n","\n","def upper_confidence_bound(mu: np.ndarray, sigma2: np.ndarray, beta: float = 2.0) -> np.ndarray:\n","    \"\"\"\n","    Upper Confidence Bound (maximize) (Task 2).\n","    UCB = mu + sqrt(beta) * sigma\n","    \"\"\"\n","    sigma = np.sqrt(np.maximum(sigma2, 0.0))\n","    return mu + np.sqrt(beta) * sigma\n","\n","# ============================================================\n","# Search space and helpers (used in Task 3 and Task 4)\n","# ============================================================\n","\n","SPACE = {\n","    \"n_estimators\": (50, 500),       # integer\n","    \"learning_rate\": (0.01, 0.2),    # float\n","    \"max_depth\": (2, 8),             # integer\n","    \"subsample\": (0.6, 1.0),         # float\n","}\n","\n","def sample_uniform(n: int) -> np.ndarray:\n","    dims = list(SPACE.keys())\n","    lows = np.array([SPACE[d][0] for d in dims], dtype=np.float64)\n","    highs = np.array([SPACE[d][1] for d in dims], dtype=np.float64)\n","    X = rng.uniform(lows, highs, size=(n, len(dims)))\n","    return X\n","\n","def array_to_params(x: np.ndarray) -> Dict[str, float]:\n","    dims = list(SPACE.keys())\n","    p = {}\n","    for i, d in enumerate(dims):\n","        low, high = SPACE[d]\n","        val = float(x[i])\n","        if d in [\"n_estimators\", \"max_depth\"]:\n","            p[d] = int(np.clip(np.round(val), low, high))\n","        else:\n","            p[d] = float(np.clip(val, low, high))\n","    return p\n","\n","def train_and_eval(params: Dict[str, float],\n","                   X_train: np.ndarray, y_train: np.ndarray,\n","                   X_val: np.ndarray, y_val: np.ndarray) -> float:\n","    \"\"\"\n","    Train GradientBoostingRegressor and return validation RMSE (minimize).\n","    Used by Task 3 and Task 4.\n","    \"\"\"\n","    model = GradientBoostingRegressor(\n","        n_estimators=int(params[\"n_estimators\"]),\n","        learning_rate=float(params[\"learning_rate\"]),\n","        max_depth=int(params[\"max_depth\"]),\n","        subsample=float(params[\"subsample\"]),\n","        random_state=RNG_SEED\n","    )\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val)\n","    # Compute RMSE manually for compatibility (no 'squared' arg)\n","    mse = mean_squared_error(y_val, y_pred)\n","    rmse = np.sqrt(mse)\n","    return rmse\n","\n","# ============================================================\n","# Task 3: Integrate GP + acquisition into Bayesian Optimization loop\n","# ============================================================\n","\n","class BayesianOptimizer:\n","    \"\"\"\n","    Bayesian Optimization loop (Task 3):\n","    - Start with n_init random evaluations.\n","    - Fit GP on f(x) = -RMSE to convert to maximization.\n","    - Sample candidate points and pick x maximizing acquisition (EI/UCB).\n","    - Evaluate, update, repeat.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        kernel_name: str = \"matern52\",\n","        lengthscale: float = 1.5,\n","        variance: float = 1.0,\n","        noise_variance: float = 1e-6,\n","        acquisition: str = \"EI\",\n","        acq_params: Dict = None,\n","        n_candidates: int = 2000,\n","    ):\n","        self.kernel_name = kernel_name\n","        self.acquisition = acquisition.upper()\n","        self.acq_params = acq_params or {}\n","        self.n_candidates = n_candidates\n","\n","        kernel_fn = matern52_kernel if kernel_name.lower() == \"matern52\" else rbf_kernel\n","        self.gp = GaussianProcess(kernel_fn, KernelParams(lengthscale, variance), noise_variance)\n","\n","        self.X_obs = None\n","        self.y_obs = None\n","\n","    def step(self, objective_fn: Callable[[Dict[str, float]], float]) -> Tuple[Dict[str, float], float]:\n","        \"\"\"\n","        One BO step:\n","        - Fit GP (Task 1)\n","        - Compute acquisition values (Task 2)\n","        - Select next params and evaluate objective\n","        \"\"\"\n","        self.gp.fit(self.X_obs, self.y_obs)\n","\n","        X_cand = sample_uniform(self.n_candidates)\n","        mu, var = self.gp.predict(X_cand, return_var=True)\n","\n","        if self.acquisition == \"EI\":\n","            best_f = float(np.max(self.y_obs))\n","            xi = float(self.acq_params.get(\"xi\", 1e-3))\n","            acq_vals = expected_improvement(mu, var, best_f, xi)  # Task 2 used here\n","        elif self.acquisition == \"UCB\":\n","            beta = float(self.acq_params.get(\"beta\", 2.0))\n","            acq_vals = upper_confidence_bound(mu, var, beta)       # Task 2 used here\n","        else:\n","            raise ValueError(\"Unknown acquisition: choose 'EI' or 'UCB'.\")\n","\n","        x_next = X_cand[int(np.argmax(acq_vals))]\n","        params = array_to_params(x_next)\n","        rmse = objective_fn(params)\n","        f_val = -rmse\n","\n","        self.X_obs = np.vstack([self.X_obs, x_next])\n","        self.y_obs = np.hstack([self.y_obs, f_val])\n","\n","        return params, rmse\n","\n","    def run(self, objective_fn: Callable[[Dict[str, float]], float], n_init: int = 5, n_iter: int = 25) -> Dict:\n","        \"\"\"\n","        Run BO for a fixed budget.\n","        Returns logs including per-iteration best RMSE.\n","        \"\"\"\n","        # Initialize random evaluations (Task 3 setup)\n","        self.X_obs = sample_uniform(n_init)\n","        init_params = [array_to_params(x) for x in self.X_obs]\n","        init_rmses = [objective_fn(p) for p in init_params]\n","        self.y_obs = -np.array(init_rmses, dtype=np.float64)\n","\n","        best_rmse = float(np.min(init_rmses))\n","        best_params = init_params[int(np.argmin(init_rmses))]\n","\n","        history = []\n","        for t in range(1, n_iter + 1):\n","            params, rmse = self.step(objective_fn)\n","            if rmse < best_rmse:\n","                best_rmse = rmse\n","                best_params = params\n","            history.append({\"iter\": t, \"params\": params, \"rmse\": rmse, \"best_rmse\": best_rmse})\n","\n","        return {\"best_params\": best_params, \"best_rmse\": best_rmse, \"history\": history}\n","\n","# ============================================================\n","# Task 4: Comparative analysis vs Random Search\n","# ============================================================\n","\n","def random_search(objective_fn: Callable[[Dict[str, float]], float], n_trials: int = 30) -> Dict:\n","    \"\"\"\n","    Random Search baseline (Task 4).\n","    \"\"\"\n","    best_rmse = np.inf\n","    best_params = None\n","    history = []\n","    for i in range(1, n_trials + 1):\n","        x = sample_uniform(1)[0]\n","        params = array_to_params(x)\n","        rmse = objective_fn(params)\n","        if rmse < best_rmse:\n","            best_rmse = rmse\n","            best_params = params\n","        history.append({\"iter\": i, \"params\": params, \"rmse\": rmse, \"best_rmse\": best_rmse})\n","    return {\"best_params\": best_params, \"best_rmse\": best_rmse, \"history\": history}\n","\n","# ============================================================\n","# Experiment driver: dataset + objective + runs\n","# Shows where each Task is used in practice\n","# ============================================================\n","\n","def main():\n","    # Dataset: high-dimensional synthetic regression\n","    X, y = make_regression(\n","        n_samples=5000,\n","        n_features=50,\n","        n_informative=40,\n","        noise=10.0,\n","        random_state=RNG_SEED\n","    )\n","\n","    # Splits\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=RNG_SEED)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=RNG_SEED)\n","\n","    # Objective closure used by Task 3 and Task 4\n","    def objective(params: Dict[str, float]) -> float:\n","        return train_and_eval(params, X_train, y_train, X_val, y_val)\n","\n","    # ----- Task 4: Random Search baseline -----\n","    rs_result = random_search(objective, n_trials=35)\n","\n","    # ----- Task 3: Bayesian Optimization with EI -----\n","    bo_ei = BayesianOptimizer(kernel_name=\"matern52\", acquisition=\"EI\", acq_params={\"xi\": 1e-3})\n","    ei_result = bo_ei.run(objective, n_init=5, n_iter=30)\n","\n","    # ----- Task 3: Bayesian Optimization with UCB -----\n","    bo_ucb = BayesianOptimizer(kernel_name=\"matern52\", acquisition=\"UCB\", acq_params={\"beta\": 2.0})\n","    ucb_result = bo_ucb.run(objective, n_init=5, n_iter=30)\n","\n","    # Evaluate best configs on test set (final deliverable summary)\n","    def eval_on_test(params: Dict[str, float]) -> float:\n","        model = GradientBoostingRegressor(\n","            n_estimators=int(params[\"n_estimators\"]),\n","            learning_rate=float(params[\"learning_rate\"]),\n","            max_depth=int(params[\"max_depth\"]),\n","            subsample=float(params[\"subsample\"]),\n","            random_state=RNG_SEED\n","        )\n","        model.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n","        y_pred = model.predict(X_test)\n","        # Compute RMSE manually for compatibility (no 'squared' arg)\n","        mse = mean_squared_error(y_test, y_pred)\n","        rmse = np.sqrt(mse)\n","        return rmse\n","\n","    rs_test_rmse = eval_on_test(rs_result[\"best_params\"])\n","    ei_test_rmse = eval_on_test(ei_result[\"best_params\"])\n","    ucb_test_rmse = eval_on_test(ucb_result[\"best_params\"])\n","\n","    # ----- Text deliverables: convergence and final summaries (Task 4 reporting) -----\n","    print(\"\\n=== Convergence (Validation RMSE) ===\")\n","\n","    def print_history(name: str, hist: List[Dict]):\n","        print(f\"\\n{name}:\")\n","        print(\"iter | rmse_current | rmse_best_so_far\")\n","        for h in hist:\n","            print(f\"{h['iter']:>4} | {h['rmse']:.4f}      | {h['best_rmse']:.4f}\")\n","\n","    print_history(\"Random Search (Task 4)\", rs_result[\"history\"])\n","    print_history(\"BayesOpt - EI (Task 3 uses Task 1 & Task 2)\", ei_result[\"history\"])\n","    print_history(\"BayesOpt - UCB (Task 3 uses Task 1 & Task 2)\", ucb_result[\"history\"])\n","\n","    print(\"\\n=== Final evaluation summary ===\")\n","    print(f\"- Random Search best val RMSE: {rs_result['best_rmse']:.4f}\")\n","    print(f\"- EI best val RMSE:            {ei_result['best_rmse']:.4f}\")\n","    print(f\"- UCB best val RMSE:           {ucb_result['best_rmse']:.4f}\")\n","\n","    print(\"\\nBest hyperparameters found:\")\n","    print(f\"- Random Search: {rs_result['best_params']}\")\n","    print(f\"- EI:             {ei_result['best_params']}\")\n","    print(f\"- UCB:            {ucb_result['best_params']}\")\n","\n","    print(\"\\nTest RMSE of best configs (trained on train+val):\")\n","    print(f\"- Random Search: {rs_test_rmse:.4f}\")\n","    print(f\"- EI:            {ei_test_rmse:.4f}\")\n","    print(f\"- UCB:           {ucb_test_rmse:.4f}\")\n","\n","    # Simple ASCII convergence curves (optional visualization)\n","    def ascii_curve(hist: List[Dict], label: str):\n","        bests = [h[\"best_rmse\"] for h in hist]\n","        mn, mx = min(bests), max(bests)\n","        rngv = mx - mn if mx > mn else 1e-12\n","        chars = \"▁▂▃▄▅▆▇█\"\n","        line = \"\".join(chars[min(int((mx - b) / rngv * (len(chars) - 1)), len(chars) - 1)] for b in bests)\n","        print(f\"{label}: {line}  (lower is better)\")\n","\n","    print(\"\\n=== ASCII convergence curves (best validation RMSE) ===\")\n","    ascii_curve(rs_result[\"history\"], \"Random Search (Task 4)\")\n","    ascii_curve(ei_result[\"history\"], \"EI (Task 3 uses Task 1 & Task 2)\")\n","    ascii_curve(ucb_result[\"history\"], \"UCB (Task 3 uses Task 1 & Task 2)\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"Hbb8Wu24vqOe","executionInfo":{"status":"ok","timestamp":1764309005420,"user_tz":-330,"elapsed":2158725,"user":{"displayName":"","userId":""}},"outputId":"a59fe84e-74ad-45cf-e457-aa73214f91df","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Convergence (Validation RMSE) ===\n","\n","Random Search (Task 4):\n","iter | rmse_current | rmse_best_so_far\n","   1 | 191.3123      | 191.3123\n","   2 | 205.2543      | 191.3123\n","   3 | 190.0970      | 190.0970\n","   4 | 155.8836      | 155.8836\n","   5 | 200.9599      | 155.8836\n","   6 | 209.6849      | 155.8836\n","   7 | 141.7224      | 141.7224\n","   8 | 182.8819      | 141.7224\n","   9 | 159.0992      | 141.7224\n","  10 | 198.2112      | 141.7224\n","  11 | 177.4863      | 141.7224\n","  12 | 137.5685      | 137.5685\n","  13 | 168.8355      | 137.5685\n","  14 | 181.7859      | 137.5685\n","  15 | 126.8894      | 126.8894\n","  16 | 153.4870      | 126.8894\n","  17 | 146.7182      | 126.8894\n","  18 | 238.7969      | 126.8894\n","  19 | 155.3343      | 126.8894\n","  20 | 163.6641      | 126.8894\n","  21 | 177.0322      | 126.8894\n","  22 | 276.0239      | 126.8894\n","  23 | 184.2075      | 126.8894\n","  24 | 150.9337      | 126.8894\n","  25 | 249.5631      | 126.8894\n","  26 | 143.1448      | 126.8894\n","  27 | 158.8294      | 126.8894\n","  28 | 164.6546      | 126.8894\n","  29 | 154.9044      | 126.8894\n","  30 | 160.0919      | 126.8894\n","  31 | 125.9193      | 125.9193\n","  32 | 205.8369      | 125.9193\n","  33 | 203.4567      | 125.9193\n","  34 | 154.1380      | 125.9193\n","  35 | 178.1587      | 125.9193\n","\n","BayesOpt - EI (Task 3 uses Task 1 & Task 2):\n","iter | rmse_current | rmse_best_so_far\n","   1 | 193.0367      | 156.9594\n","   2 | 157.5728      | 156.9594\n","   3 | 148.6061      | 148.6061\n","   4 | 245.2572      | 148.6061\n","   5 | 148.0714      | 148.0714\n","   6 | 190.8894      | 148.0714\n","   7 | 205.3884      | 148.0714\n","   8 | 158.6465      | 148.0714\n","   9 | 276.3560      | 148.0714\n","  10 | 194.7175      | 148.0714\n","  11 | 206.6125      | 148.0714\n","  12 | 206.0445      | 148.0714\n","  13 | 148.1706      | 148.0714\n","  14 | 191.8236      | 148.0714\n","  15 | 226.8048      | 148.0714\n","  16 | 146.5748      | 146.5748\n","  17 | 175.5681      | 146.5748\n","  18 | 118.5987      | 118.5987\n","  19 | 156.6940      | 118.5987\n","  20 | 162.0438      | 118.5987\n","  21 | 215.2891      | 118.5987\n","  22 | 272.2524      | 118.5987\n","  23 | 140.2915      | 118.5987\n","  24 | 252.5549      | 118.5987\n","  25 | 121.6569      | 118.5987\n","  26 | 139.4918      | 118.5987\n","  27 | 183.6253      | 118.5987\n","  28 | 209.9847      | 118.5987\n","  29 | 204.1560      | 118.5987\n","  30 | 199.3518      | 118.5987\n","\n","BayesOpt - UCB (Task 3 uses Task 1 & Task 2):\n","iter | rmse_current | rmse_best_so_far\n","   1 | 166.2275      | 128.1176\n","   2 | 116.6792      | 116.6792\n","   3 | 146.5938      | 116.6792\n","   4 | 179.7374      | 116.6792\n","   5 | 192.2287      | 116.6792\n","   6 | 242.9836      | 116.6792\n","   7 | 152.7252      | 116.6792\n","   8 | 195.6160      | 116.6792\n","   9 | 193.4205      | 116.6792\n","  10 | 214.3261      | 116.6792\n","  11 | 188.8179      | 116.6792\n","  12 | 183.7791      | 116.6792\n","  13 | 130.0005      | 116.6792\n","  14 | 115.7962      | 115.7962\n","  15 | 189.9507      | 115.7962\n","  16 | 115.9898      | 115.7962\n","  17 | 143.0215      | 115.7962\n","  18 | 182.8686      | 115.7962\n","  19 | 201.3718      | 115.7962\n","  20 | 145.7799      | 115.7962\n","  21 | 267.3326      | 115.7962\n","  22 | 197.0191      | 115.7962\n","  23 | 204.9129      | 115.7962\n","  24 | 245.4006      | 115.7962\n","  25 | 218.4275      | 115.7962\n","  26 | 192.9089      | 115.7962\n","  27 | 195.6651      | 115.7962\n","  28 | 127.8795      | 115.7962\n","  29 | 117.5654      | 115.7962\n","  30 | 179.7902      | 115.7962\n","\n","=== Final evaluation summary ===\n","- Random Search best val RMSE: 125.9193\n","- EI best val RMSE:            118.5987\n","- UCB best val RMSE:           115.7962\n","\n","Best hyperparameters found:\n","- Random Search: {'n_estimators': 313, 'learning_rate': 0.1334708542954158, 'max_depth': 3, 'subsample': 0.7663229608682438}\n","- EI:             {'n_estimators': 481, 'learning_rate': 0.13712991819927187, 'max_depth': 3, 'subsample': 0.9339505142342344}\n","- UCB:            {'n_estimators': 438, 'learning_rate': 0.12236701884561857, 'max_depth': 3, 'subsample': 0.8175864069436185}\n","\n","Test RMSE of best configs (trained on train+val):\n","- Random Search: 121.6642\n","- EI:            119.4807\n","- UCB:           114.7640\n","\n","=== ASCII convergence curves (best validation RMSE) ===\n","Random Search (Task 4): ▁▁▁▄▄▄▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████  (lower is better)\n","EI (Task 3 uses Task 1 & Task 2): ▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█████████████  (lower is better)\n","UCB (Task 3 uses Task 1 & Task 2): ▁▇▇▇▇▇▇▇▇▇▇▇▇█████████████████  (lower is better)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"B5yfgG2yvxXg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1764309067966}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}